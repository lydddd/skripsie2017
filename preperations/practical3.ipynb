{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeDW4HuegASG"
   },
   "source": [
    "# DL Indaba Practical 3\n",
    "# Convolutional Neural Networks\n",
    "*Developed by Stephan Gouws, Avishkar Bhoopchand & Ulrich Paquet.*\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this practical we will cover the basics of convolutional neural networks, or \"ConvNets\". ConvNets were invented in the late 1980s/early 1990s, and have had tremendous success especially with vision (although they have also been used to great success in speech processing pipelines, and more recently, for machine translation). \n",
    "\n",
    "We will work to build our mathematical and algorithmic intuition around the \"convolution\" operation. Then we will construct a deep feedforward convolutional model with which we can classify MNIST digits with over 99% accuracy (our best model yet!).\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "Understand:\n",
    "* what a convolutional layer is & how it's different from a fully-connected layer (including the assumptions and trade-offs that are being made), \n",
    "* how and when to use convolutional layers (relate it to the assumptions the model makes),\n",
    "* how backpropagation works through convolutional layers.\n",
    "\n",
    "**What is expected of you:**\n",
    "\n",
    "* Read through the explanations and make sure you understand how to implement the convolutional forwards pass.\n",
    "* Do the same for the backwards phase.\n",
    "* Train a small model on MNIST.\n",
    "* At this point, flag a tutor and they will give you access to a GPU instance. Now use the hyperparameters provided to train a state-of-the-art ConvNet model on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 153,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13901,
     "status": "ok",
     "timestamp": 1504366824744,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "p1tyOwH9Hiwa",
    "outputId": "f05866a1-64be-4d1f-9dcd-589c72e6eeb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and some other libraries we'll be using.\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import Matplotlib and set some defaults\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ioff()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Download the MNIST dataset onto the local machine.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "za-6kOBMQuWg"
   },
   "source": [
    "# ConvNet Architectures\n",
    "When modelling an image using a regular feed-forward network, we quickly find that the number of model parameters grows exponentially. For example, our 2 layer MNIST feed-forward model from the previous practical already had over 600 000 parameters!\n",
    "\n",
    "**QUESTION**: How many parameters would a feed-forward network require if it had 2 hidden layers with 512 and 256 neurons respectively, an output size of 10 and an input image of shape [32, 32, 3]?\n",
    "\n",
    "ConvNets address this model parameter issue by exploiting structure in the inputs to the network (in particular, by making the assumption that the input is a 3D volume, which applies to images for example). The two key differences between a ConvNet and a Feed-forward network are:\n",
    "* ConvNets have neurons that are arranged in 3 dimensions: width, height, depth (depth here means the depth of an activation volume, not the depth of a deep neural network!)\n",
    "* The neurons in each layer are only connected to a small region of the layer before it.\n",
    "\n",
    "**QUESTION**: Unfortunately there is no such thing as a free lunch. What do you think is the trade-off a ConvNet makes for the reduction in memory required by fewer parameters? \n",
    "\n",
    "Generally a ConvNet architecture is made up of different types of layers, the most common being **convolutional layers**, **pooling layers** and **fully connected layers** that we encountered in the last practical. \n",
    "\n",
    "ConvNet architectures were key to the tremendous success of deep learning in machine vision. In particular, the first deep learning model to win the ImageNet competition in 2012 was called AlexNet (after Alex Krizhevsky, one of its inventors). It had 5 convolutional layers followed by 3 fully connected layers. Later winners included GoogLeNet and ResNet which also used batch normalisation, a technique we will see in this practical. If you're curious, have a look at [this](https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba) link for a great summary of different ConvNet archiectures. \n",
    "\n",
    "We will start by implementing the forward and backward passes of these layers in Numpy to get a good sense for how they work. Afterwards, we will implement a full ConvNet classifier in TensorFlow that we will apply to the MNIST dataset. This model should give us the best test accuracy we've seen so far! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3-Aje8WEyfA"
   },
   "source": [
    "## Convolutional Layers\n",
    "A convolutional layer maps an **input volume**\\* to an **output volume** through a set of **learnable filters**, which make up the parameters of the layer. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. (Eg: A filter in the first layer of a ConvNet might have size [5, 5, 3]). During the forward pass, we convolve (\"slide\") each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Each convolutional layer will have a set of filters, and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension to produce the output volume. \n",
    "\n",
    "\n",
    "The following diagram and animation illustrates these ideas, make sure you understand them!\n",
    "\n",
    "![Convolutional Layer](images/conv_layer.png)\n",
    "![Animated Convolutional Layer](images/conv_animation.png)\n",
    "\n",
    "\\* An input volume refers to a 3 dimensional input. For example, a colour image is often represented as a 3 dimensional tensor of shape `[width, height, channels]` where `channels` refers to the colour values. A common colour encoding is RGB which has a value between 0 and 256 for each of the red, green and blue channels.\n",
    "\n",
    "### What size is the output volume?\n",
    "The size of the output volume is controlled by the hyperparameters of the convolutional layer:\n",
    "* **Filter Size** (F) defines the width and height of the filters in the layer. Note that filters always have the same depth as the inputs to the layer.    \n",
    "* **Depth** (D) of the layer defines the number of filters in the layer. \n",
    "* **Stride** (S) defines the number of pixels by which we move the filter when \"sliding\" it along the input volume. Typically this value would be 1, but values of 2 and 3 are also sometimes used. \n",
    "* **Padding** (P) refers to the number of 0 pixels we add to the input volume along the width and height dimensions. This parameter is useful in that it gives us more control over the desired size of the output volume and in fact is often used to ensure that the output volume has the same width and height as the input volume. \n",
    "\n",
    "If the width of the input volume is $w$, the width of the output volume will be $(wâˆ’F+2P)/S+1$. (**QUESTION:** Why?). Similarly for the height ($h$). \n",
    "\n",
    "**QUESTION**: What is the final 3D shape of the output volume? \n",
    "\n",
    "### Implementing the forward pass\n",
    "The parameters of a convolutional layer, with padded input $X^{pad}$, are stored in a weight tensor, $W$ of shape $[F, F, I, D]$ and bias vector $b$ of shape $[D]$ where I is the depth of $X$. \n",
    "For each filter $d \\in [0,D)$ in our convolutional layer, the value of the output volume ($O$) at position $(i, j, d)$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "O_{ij}^d = b_{d} + \\sum_{a=0}^{F-1} \\sum_{b=0}^{F-1} \\sum_{c=0}^{I-1} W_{a, b, c, d} X^{pad}_{i+a, j+b, c}    && (1)\n",
    "\\end{align}\n",
    "\n",
    "Don't be put off by all the notation, it's actually quite simple, see if you can tie this formula to the explanation of the convolutional layer and diagrams you saw earlier. \n",
    "\n",
    "**QUESTION**: The formula above assumed a stride size of 1 for simplicity. Can you modify the formula to work with an arbitrary stride?\n",
    "\n",
    "Now let's implement the forward pass of a convolutional layer in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "nynzAwx2_nHl"
   },
   "outputs": [],
   "source": [
    "## IMPLEMENT-ME: ...\n",
    "\n",
    "# Conv layer forward pass\n",
    "def convolutional_forward(X, W, b, filter_size, depth, stride, padding):\n",
    "    # X has size [batch_size, input_width, input_height, input_depth]\n",
    "    # W has shape [filter_size, filter_size, input_depth, depth]\n",
    "    # b has shape [depth]\n",
    "    batch_size, input_width, input_height, input_depth = X.shape\n",
    "    \n",
    "    # Check that the weights are of the expected shape\n",
    "    assert W.shape == (filter_size, filter_size, input_depth, depth)\n",
    "    \n",
    "    # QUESTION: Calculate the width and height of the output\n",
    "    # output_width = ...\n",
    "    # output_height = ...\n",
    "    #\n",
    "    # ANSWER:\n",
    "    output_width = (input_width - filter_size + 2*padding) / stride + 1\n",
    "    output_height = (input_height - filter_size + 2*padding) / stride + 1\n",
    "    ####\n",
    "    \n",
    "    # Apply padding to the width and height dimensions of the input\n",
    "    X_padded = np.pad(X, ((0,0), (padding, padding), (padding, padding), (0,0)), 'constant')\n",
    "    \n",
    "    # Allocate the output Tensor\n",
    "    out = np.zeros((batch_size, output_width, output_height, depth))\n",
    "    \n",
    "    # NOTE: There is a more efficient way of doing a convolution, but this most \n",
    "    # clearly illustrates the idea.\n",
    "    \n",
    "    for i in range(output_width):    # Loop over the output width dimension\n",
    "        for j in range(output_height):    # Loop over the output height dimension\n",
    "            \n",
    "            # Select the current block in the input that the filter will be applied to \n",
    "            block_width_start = i * stride\n",
    "            block_width_end = block_width_start + filter_size\n",
    "\n",
    "            block_height_start = j * stride\n",
    "            block_height_end = block_height_start + filter_size\n",
    "\n",
    "            block = X_padded[:, block_width_start:block_width_end, block_height_start:block_height_end, :]\n",
    "            \n",
    "            for d in range(depth):    # Loop over the filters in the layer (output depth dimension)\n",
    "                \n",
    "                filter_weights = W[:, :, :, d]\n",
    "                # QUESTION: Apply the filter to the block over all inputs in the batch\n",
    "                # out[:, w, h, f] = ...\n",
    "                # HINT: Have a look at numpy's sum function and pay attention to the axis parameter\n",
    "                # ANSWER:\n",
    "                out[:, i, j, d] = np.sum(block * filter_weights, axis=(1,2,3)) + b[d]\n",
    "                ###\n",
    "            \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68fLAdyYFOGF"
   },
   "source": [
    "Let's test our layer on some dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1504358221278,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "uG7LunQ6FNnH",
    "outputId": "2d84bea9-1ebd-4e98-824e-7b4ea2c017a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing convolutional_forward\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "batch_size = 2\n",
    "input_width = 4\n",
    "input_height = 4\n",
    "input_depth = 3\n",
    "filter_size = 4\n",
    "output_depth = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "###\n",
    "\n",
    "# Create a helper function that calculates the relative error between two arrays\n",
    "def relative_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "# Define the shapes of the input and weights\n",
    "input_shape = (batch_size, input_width, input_height, input_depth)\n",
    "w_shape = (filter_size, filter_size, input_depth, output_depth)\n",
    "\n",
    "# Create the dummy input\n",
    "X = np.linspace(-0.1, 0.5, num=np.prod(input_shape)).reshape(input_shape)\n",
    "\n",
    "# Create the weights and biases\n",
    "W = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=output_depth)\n",
    "\n",
    "# Get the output of the convolutional layer\n",
    "out = convolutional_forward(X, W, b, filter_size, output_depth, stride, padding)\n",
    "\n",
    "correct_out = np.array(\n",
    "        [[[[8.72013250e-02, 2.37300699e-01, 3.87400074e-01],\n",
    "             [1.34245123e-01, 2.86133235e-01, 4.38021347e-01]],\n",
    "            [[8.21928598e-02, 2.39447184e-01, 3.96701509e-01],\n",
    "             [4.47552448e-04, 1.59490615e-01, 3.18533677e-01]]],\n",
    "         [[[1.11179021e+00, 1.29050939e+00, 1.46922856e+00],\n",
    "             [9.01255797e-01, 1.08176371e+00, 1.26227162e+00]],\n",
    "            [[7.64688995e-02, 2.62343025e-01, 4.48217151e-01],\n",
    "             [-2.62854619e-01, -7.51917556e-02, 1.12471108e-01]]]])\n",
    "\n",
    "# Compare your output to the \"correct\" ones \n",
    "# The difference should be around 2e-8 (or lower)\n",
    "\n",
    "print 'Testing convolutional_forward'\n",
    "diff = relative_error(out, correct_out)\n",
    "if diff <= 2e-8:\n",
    "    print 'PASSED'\n",
    "else:\n",
    "    print 'The difference of %s is too high, try again' % diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-3eBUD6E_Ph"
   },
   "source": [
    "### The derivative of a convolutional layer\n",
    "Assume we have some final loss function L and by following the steps of backpropagation, have computed the derivative of this loss up to the output of our convolutional layer ($\\frac{\\partial L}{\\partial O}$ or `dout` in the code below). In order to update the parameters of our layer, we require the derivative of L with respect to the weights and biases of the convolutional layer ($\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$). We also require the derivative with respect to the inputs of the layer ($\\frac{\\partial L}{\\partial X}$) in order to propagate the error back to the preceding layers. Unfortunately calculating these derivatives can be a little fiddly due to having to keep track of multiple indices. The calculus is very basic though!\n",
    "\n",
    "We start with the easiest one, $\\frac{\\partial L}{\\partial b}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial O} \\frac{\\partial O}{\\partial b} && \\vartriangleright \\text{(Chain Rule)} \\\\\n",
    " &= \\frac{\\partial L}{\\partial O} \\mathbf{1} && \\vartriangleright (\\frac{\\partial O}{\\partial b} = 1 \\text{ from equation } (1))\n",
    "\\end{align}\n",
    "\n",
    "Now we tackle $\\frac{\\partial L}{\\partial W}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W} &= \\frac{\\partial L}{\\partial O} \\frac{\\partial O}{\\partial W} && \\vartriangleright \\text{(Chain Rule)}\n",
    "\\end{align}\n",
    "\n",
    "Let's calculate this derivative with respect to a single point $W_{abcd}$ in our weight tensor ($O_w$ and $O_h$ are the output width and height respectively):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W_{abcd}} &= \\sum_{i=0}^{O_w-1} \\sum_{j=0}^{O_h-1} \\frac{\\partial L}{\\partial O_{ij}^d} \\frac{\\partial O_{ij}^d}{\\partial W_{abcd}}\n",
    "\\end{align}\n",
    "\n",
    "**QUESTION**: Why do we sum over the outputs here? **HINT**: Think about how many times a particular weight gets used. \n",
    "\n",
    "Now, looking at equation $(1)$, we can easily calculate $\\frac{\\partial O_{ij}^d}{\\partial W_{abcd}}$ as: \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial O_{ij}^d}{\\partial W_{abcd}} &= X^{pad}_{i+a, j+b, c}\n",
    "\\end{align}\n",
    "\n",
    "Which gives a final result of:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W_{abcd}} &= \\sum_{i=0}^{O_w-1} \\sum_{j=0}^{O_h-1} \\frac{\\partial L}{\\partial O_{ij}^d} X^{pad}_{i+a, j+b, c}\n",
    "\\end{align}\n",
    "\n",
    "Finally, we need $\\frac{\\partial L}{\\partial X}$, the derivative of the loss with respect to the **input** of the layer. This is sometimes also called a \"delta\". Remember, that before doing the convolution, we applied padding to the input $X$ to get $X^{pad}$. It's easier to calculate the derivative with respect to $X^{pad}$, which appears in our convolution equation, and then remove the padding later on to arrive at the delta. Unfortunately we need to introduce some *more* indexing for the individual components of $X^{pad}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial X^{pad}_{mnc}} &= \\sum_{i=0}^{O_w-1} \\sum_{j=0}^{O_h-1} \\sum_{d=0}^{D-1} W_{m-i, n-j, c, d} \\frac{\\partial L}{\\partial O_{ij}^d} \n",
    "\\end{align}\n",
    "\n",
    "Where do the indices $m-i$ and $n-j$ come from? Notice in equation $(1)$ that the padded input $X^{pad}_{i+a, j+b, c}$ is multiplied by the weight $W_{abcd}$. Now, when we index $X^{pad}$ with $m$ and $n$, setting $m=i+a$ and $n=j+b$ gives us $a=m-i$ and $b=n-j$ for the indices of $W$! \n",
    "\n",
    "Phew! Spend a few minutes to understand these equations, particularly where the indices come from. Ask a tutor if you get stuck!\n",
    "\n",
    "Note: Did you notice that the delta, $\\frac{\\partial L}{\\partial X^{pad}_{mnc}}$ looks suspiciously like the convolutional forward equation with the inputs $X^{pad}$ replaced by $\\frac{\\partial L}{\\partial O_{ij}^d}$ and different indexing into the weights? In fact the delta is exactly that, the forward convolution applied to the incoming derivative, with the filters flipped along the width and height axes. \n",
    "\n",
    "Now let's implement this in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "pF56E8-AFHs7"
   },
   "outputs": [],
   "source": [
    "## IMPLEMENT-ME: ...\n",
    "\n",
    "def convolutional_backward(dout, X, W, b, filter_size, depth, stride, padding):\n",
    "    batch_size, input_width, input_height, input_depth = X.shape\n",
    "    \n",
    "    # Apply padding to the width and height dimensions of the input\n",
    "    X_padded = np.pad(X, ((0,0), (padding, padding), (padding, padding), (0,0)), 'constant')\n",
    "    \n",
    "    # Calculate the width and height of the forward pass output\n",
    "    output_width = (input_width - filter_size + 2*padding) / stride + 1\n",
    "    output_height = (input_height - filter_size + 2*padding) / stride + 1\n",
    "    \n",
    "    # Allocate output arrays\n",
    "    # QUESTION: What is the shape of dx? dw? db?\n",
    "    # ANSWER: ...\n",
    "    dx_padded = np.zeros_like(X_padded) \n",
    "    dw = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "    \n",
    "    # QUESTION: Calculate db, the derivative of the final loss with respect to the bias term\n",
    "    # HINT: Have a look at the axis parameter of the np.sum function.\n",
    "    db = np.sum(dout, axis = (0, 1, 2))\n",
    "    \n",
    "    for i in range(output_width):\n",
    "        for j in range(output_height):\n",
    "            \n",
    "            # Select the current block in the input that the filter will be applied to \n",
    "            block_width_start = i*stride\n",
    "            block_width_end = block_width_start+filter_size\n",
    "\n",
    "            block_height_start = j*stride\n",
    "            block_height_end = block_height_start + filter_size\n",
    "\n",
    "            block = X_padded[:, block_width_start:block_width_end, block_height_start:block_height_end, :]\n",
    "            \n",
    "            for d in range(depth):\n",
    "                # QUESTION: Calculate dw[:,:,:,f], the derivative of the loss with respect to the weight parameters of the f'th filter.\n",
    "                # HINT: You can do this in a loop if you prefer, or use np.sum and \"None\" indexing to get your result to the correct \n",
    "                # shape to assign to dw[:,:,:,f], see (https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#numpy.newaxis)\n",
    "                dw[:,:,:,d] += np.sum(block*(dout[:,i,j,d])[:,None,None,None], axis=0)\n",
    "                \n",
    "            dx_padded[:,block_width_start:block_width_end, block_height_start:block_height_end, :] += np.einsum('ij,klmj->iklm', dout[:,i,j,:], W)\n",
    "            \n",
    "        # Now we remove the padding to arrive at dx\n",
    "        dx = dx_padded[:,padding:-padding, padding:-padding, :]\n",
    "        \n",
    "    return dx, dw, db\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqzwcoBpjcZI"
   },
   "source": [
    "Finally, we test the backward pass using numerical gradient checking. This compares the gradients generated by our backward function, with a numerical approximation obtained by treating our forward function as a \"black box\". This gradient checking is a very important testing tool when building your own neural network components or back-propagation system! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 85,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1503655297426,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "-L-Gl3B7jegR",
    "outputId": "eee96a03-5205-4c54-9ab6-5100b395fbd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx check: PASSED\n",
      "dw check: PASSED\n",
      "db check: PASSED\n"
     ]
    }
   ],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "        \"\"\"\n",
    "        Evaluate a numeric gradient for a function that accepts a numpy\n",
    "        array and returns a numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # QUESTION: Can you describe intuitively what this function is doing? \n",
    "        \n",
    "        grad = np.zeros_like(x)\n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "\n",
    "            oldval = x[ix]\n",
    "            x[ix] = oldval + h\n",
    "            pos = f(x).copy()\n",
    "            x[ix] = oldval - h\n",
    "            neg = f(x).copy()\n",
    "            x[ix] = oldval\n",
    "\n",
    "            grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "            it.iternext()\n",
    "        return grad\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "# Normally, backpropagation will have calculated a derivative of the final loss with respect to \n",
    "# the output of our layer. Since we're testing our layer in isolation here, we'll just pretend\n",
    "# and use a random value\n",
    "dout = np.random.randn(2, 2, 2, 3)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: convolutional_forward(X, W, b, filter_size, output_depth, stride, padding), X, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: convolutional_forward(X, W, b, filter_size, output_depth, stride, padding), W, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: convolutional_forward(X, W, b, filter_size, output_depth, stride, padding), b, dout)\n",
    "\n",
    "out = convolutional_forward(X, W, b, filter_size, output_depth, stride, padding)\n",
    "dx, dw, db = convolutional_backward(dout, X, W, b, filter_size, output_depth, stride, padding)\n",
    "\n",
    "# Your errors should be around 1e-8'\n",
    "print('Testing conv_backward_naive function')\n",
    "\n",
    "dx_diff = relative_error(dx, dx_num)\n",
    "if dx_diff < 1e-8:\n",
    "    print 'dx check: PASSED'\n",
    "else:\n",
    "    print 'The difference of %s on dx is too high, try again!' % dx_diff\n",
    "    \n",
    "dw_diff = relative_error(dw, dw_num)\n",
    "if dw_diff < 1e-8:\n",
    "    print 'dw check: PASSED'\n",
    "else:\n",
    "    print 'The difference of %s on dw is too high, try again!' % dw_diff\n",
    "    \n",
    "db_diff = relative_error(db, db_num)\n",
    "if db_diff < 1e-8:\n",
    "    print 'db check: PASSED'\n",
    "else:\n",
    "    print 'The difference of %s on db is too high, try again!' % db_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bczw3pcaJxh_"
   },
   "source": [
    "## (Max) Pooling Layers\n",
    "The purpose of a pooling layer is to is to reduce the spatial size of the representation and therefore control the number of parameters in the network. A pooling layer has no trainable parameters itself. It applies some 2D aggegation operation (usually a MAX, but others like average may also be used) to regions of the input volume. This is done independently for each depth dimension of the input. For example, a 2x2 max pooling operation with a stride of 2, downsamples every depth slice of the input by 2 along both the width and height. \n",
    "\n",
    "The output volume of a pooling layer alwyas has the same depth as the input volume. The width and height are calcualted as follows:\n",
    "$(Wâˆ’F)/S+1$ where W is the width/height of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdEN_ur1Kv1x"
   },
   "source": [
    "### Implementing the forward pass\n",
    "We again implement this in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QgowKDV6KzOJ"
   },
   "outputs": [],
   "source": [
    "def max_pool_forward(X, pool_size, stride):\n",
    "    batch_size, input_width, input_height, input_depth = X.shape\n",
    "    \n",
    "    # Calculate the output dimensions\n",
    "    output_width = (input_width - pool_size)/stride + 1\n",
    "    output_height = (input_height - pool_size)/stride + 1\n",
    "    \n",
    "    # Allocate the output array\n",
    "    out = np.zeros((batch_size, output_width, output_height, input_depth))\n",
    "    \n",
    "    # Select the current block in the input that the filter will be applied to \n",
    "    for w in range(output_width):\n",
    "        for h in range(output_height):\n",
    "            block_width_start = w*stride\n",
    "            block_width_end = block_width_start+pool_size\n",
    "\n",
    "            block_height_start = h*stride\n",
    "            block_height_end = block_height_start + pool_size\n",
    "\n",
    "            block = X[:, block_width_start:block_width_end, block_height_start:block_height_end, :]\n",
    "            ## IMPLEMENT-ME CANDIDATE\n",
    "            out[:,w,h,:] = np.max(block, axis=(1,2))\n",
    "            \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmIkFclN9Hxz"
   },
   "source": [
    "Now we can test the `max_pool_forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1503655319186,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "9DMjg7hf9NoM",
    "outputId": "0bcd66eb-207f-45c2-80d1-36c4e0da9524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward function:\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "batch_size = 2\n",
    "input_width = 4\n",
    "input_height = 4\n",
    "input_depth = 3\n",
    "pool_size = 2\n",
    "stride = 2\n",
    "###\n",
    "\n",
    "input_shape = (batch_size, input_width, input_height, input_depth)\n",
    "X = np.linspace(-0.3, 0.4, num=np.prod(input_shape)).reshape(input_shape)\n",
    "\n",
    "out = max_pool_forward(X, pool_size, stride)\n",
    "\n",
    "correct_out = np.array([\n",
    "        [[[-0.18947368, -0.18210526, -0.17473684],\n",
    "            [-0.14526316, -0.13789474, -0.13052632]],\n",
    "         [[-0.01263158, -0.00526316, 0.00210526],\n",
    "            [0.03157895, 0.03894737, 0.04631579]]],\n",
    "        [[[0.16421053, 0.17157895, 0.17894737],\n",
    "            [0.20842105, 0.21578947, 0.22315789]],\n",
    "         [[0.34105263, 0.34842105, 0.35578947],\n",
    "            [0.38526316, 0.39263158, 0.4]]]])\n",
    "\n",
    "# Compare the output. The difference should be less than 1e-6.\n",
    "print('Testing max_pool_forward function:')\n",
    "diff = relative_error(out, correct_out)\n",
    "if diff < 1e-6:\n",
    "    print 'PASSED'\n",
    "else:\n",
    "    print 'The difference of %s is too high, try again!' % diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f09Zws5lK1W7"
   },
   "source": [
    "### The derivative of a max-pool layer\n",
    "\n",
    "The max-pooling layer has no learnable parameters of its own, so the only derivative of concern is that of the output of the layer with respect to the input for the purpose of backpropagating the error through the layer. This is easy to calculate as it only requires that we recalculate (or remember) which value in each block was the maximum. Since each output depends only on one value in some FxF block of the input, the gradients of the max-pool layer will be sparse. \n",
    "\n",
    "Let's implement the backward pass in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "n_thklIPLDoV"
   },
   "outputs": [],
   "source": [
    "def max_pool_backward(dout, X, max_pool_output, pool_size, stride):\n",
    "    batch_size, input_width, input_height, input_depth = X.shape\n",
    "    \n",
    "    # Calculate the output dimensions\n",
    "    output_width = (input_width - pool_size)/stride + 1\n",
    "    output_height = (input_height - pool_size)/stride + 1\n",
    "    \n",
    "    # QUESTION: What is the size of dx, the derivative with respect to x? \n",
    "    # Allocate an array to hold the derivative\n",
    "    dx = np.zeros_like(X)\n",
    "    \n",
    "    for w in range(output_width):\n",
    "        for h in range(output_height):\n",
    "            # Which block in the input did the value at the forward pass output come from?\n",
    "            block_width_start = w*stride\n",
    "            block_width_end = block_width_start+pool_size\n",
    "\n",
    "            block_height_start = h*stride\n",
    "            block_height_end = block_height_start + pool_size\n",
    "\n",
    "            block = X[:, block_width_start:block_width_end, block_height_start:block_height_end, :]\n",
    "            \n",
    "            # What was the maximum value\n",
    "            max_val = max_pool_output[:, w, h, :]\n",
    "            \n",
    "            # Which values in the input block resulted in the output?\n",
    "            responsible_values = block == max_val[:, None, None, :]\n",
    "            \n",
    "            # Add the contribution of the current block to the gradient\n",
    "            dx[:,block_width_start:block_width_end,block_height_start:block_height_end, :] += responsible_values * (dout[:,w,h,:])[:,None,None,:]\n",
    "            \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4rjVDGy8H3S"
   },
   "source": [
    "And we again use numerical gradient checking to ensure that the backward function is correct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1503655384343,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "EOATBv5Q8M9R",
    "outputId": "271fc91a-4c78-4def-a2df-8bc1f29d10f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward function:\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "# Define a hypothetical derivative of the loss function with respect to the output of the max-pooling layer.\n",
    "dout = np.random.randn(batch_size, pool_size, pool_size, input_depth)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward(x, pool_size, stride), X, dout)\n",
    "out = max_pool_forward(X, pool_size, stride)\n",
    "dx = max_pool_backward(dout, X, out, pool_size, stride)\n",
    "\n",
    "# Your error should be less than 1e-12\n",
    "print('Testing max_pool_backward function:')\n",
    "diff = relative_error(dx, dx_num)\n",
    "if diff < 1e-12:\n",
    "    print 'PASSED'\n",
    "else:\n",
    "    print 'The diff of %s is too large, try again!' % diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59ZgyinR66B-"
   },
   "source": [
    "## Optimisation - an exercise for later\n",
    "Our implementations of convolutional and max-pool layers were based on loops, which are easy to understand, but are slow and inefficient compared to a vectorised implementation exploiting matrix multiplications. The vectorised form is how these layers are actually implemented in practice and are also required to make efficient use of GPUs in frameworks that support it, like TensorFlow. As an exercise, once you fully understand how the layers work, try to rewrite the code such that the convolution and max-pool operations are each implemented in a **single matrix multiplication**. \n",
    "\n",
    "(HINT: Matlab has a function called \"im2col\" that rearranges blocks of an image into columns, you will need to achieve something similar using Numpy!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CyCc4zdntUe"
   },
   "source": [
    "# Building a 2-layer ConvNet in TensorFlow\n",
    "\n",
    "Now that we understand the convolutional and max pool layers, let's switch back to TensorFlow and build a 2-layer ConvNet classifier that we can apply to MNIST. We reuse essentially the same classifier framework we used in Practical 2 as well as the training and plotting functions, but we have added support for 2 new forms of regularisation, **dropout** and **batch normalisation**. These are explained in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "gTABb64k3eiW"
   },
   "outputs": [],
   "source": [
    "class BaseSoftmaxClassifier(object):\n",
    "    def __init__(self, input_size, output_size):        \n",
    "        # Define the input placeholders. The \"None\" dimension means that the \n",
    "        # placeholder can take any number of images as the batch size. \n",
    "        self.x = tf.placeholder(tf.float32, [None, input_size])\n",
    "        self.y = tf.placeholder(tf.float32, [None, output_size])    \n",
    "        \n",
    "        # We add an additional input placeholder for Dropout regularisation\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "        \n",
    "        # And one for bath norm regularisation\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # You should override these in your build_model() function.\n",
    "        self.logits = None\n",
    "        self.predictions = None\n",
    "        self.loss = None\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def get_logits(self):\n",
    "        return self.logits\n",
    "    \n",
    "    def build_model(self):\n",
    "        # OVERRIDE THIS FOR YOUR PARTICULAR MODEL.\n",
    "        raise NotImplementedError(\"Subclasses should implement this function!\")\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        \"\"\"All models share the same softmax cross-entropy loss.\"\"\"\n",
    "        assert self.logits is not None    # Ensure that logits has been created! \n",
    "        data_loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "            \n",
    "        return data_loss\n",
    "    \n",
    "    def accuracy(self):\n",
    "        # Calculate accuracy.\n",
    "        assert self.predictions is not None    # Ensure that pred has been created!\n",
    "        correct_prediction = tf.equal(tf.argmax(self.predictions, 1), tf.argmax(self.y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wk8Vo4mzGvz6"
   },
   "source": [
    "Lets also bring in the training and plotting routines we developed in Prac 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "EAgod2aDGvFn"
   },
   "outputs": [],
   "source": [
    "def train_tf_model(tf_model,                                     \n",
    "                   session,    # The active session.\n",
    "                   num_epochs,    # Max epochs/iterations to train for.\n",
    "                   batch_size=100,    # Number of examples per batch.\n",
    "                   keep_prob=1.0,    # (1. - dropout) probability, none by default.\n",
    "                   optimizer_fn=None,    # TODO(sgouws): more correct to call this optimizer_obj\n",
    "                   report_every=1, # Report training results every nr of epochs.\n",
    "                   eval_every=1,    # Evaluate on validation data every nr of epochs.\n",
    "                   stop_early=True,    # Use early stopping or not.\n",
    "                   verbose=True): \n",
    "\n",
    "    # Get the (symbolic) model input, output, loss and accuracy.\n",
    "    x, y = tf_model.x, tf_model.y\n",
    "    loss = tf_model.loss\n",
    "    accuracy = tf_model.accuracy()\n",
    "\n",
    "    # Compute the gradient of the loss with respect to the model parameters \n",
    "    # and create an op that will perform one parameter update using the specific\n",
    "    # optimizer's update rule in the direction of the gradients.\n",
    "    if optimizer_fn is None:\n",
    "        optimizer_fn = tf.train.AdamOptimizer(1e-4)\n",
    "        \n",
    "    # For batch normalisation: Ensure that the mean and variance tracking \n",
    "    # variables get updated at each training step\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer_step = optimizer_fn.minimize(loss)\n",
    "\n",
    "    # Get the op which, when executed, will initialize the variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Actually initialize the variables (run the op).\n",
    "    session.run(init)\n",
    "\n",
    "    # Save the training loss and accuracies on training and validation data.\n",
    "    train_costs = []\n",
    "    train_accs = []\n",
    "    val_costs = []\n",
    "    val_accs = []\n",
    "\n",
    "    mnist_train_data = mnist.train\n",
    "    \n",
    "    prev_c_eval = 1000000\n",
    "    \n",
    "    # Main training cycle.\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches.\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist_train_data.next_batch(batch_size)\n",
    "                        \n",
    "            # Run optimization op (backprop) and cost op (to get loss value),\n",
    "            # and compute the accuracy of the model.\n",
    "            feed_dict = {x: batch_x, y: batch_y, tf_model.keep_prob: keep_prob,\n",
    "                                    tf_model.is_training: True}\n",
    "            _, c, a = session.run(\n",
    "                    [optimizer_step, loss, accuracy], feed_dict=feed_dict)\n",
    "                        \n",
    "            # Compute average loss/accuracy\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += a / total_batch            \n",
    "        \n",
    "        train_costs.append((epoch, avg_cost))\n",
    "        train_accs.append((epoch, avg_acc))\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % report_every == 0 and verbose:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"Training cost=\", \\\n",
    "                        \"{:.9f}\".format(avg_cost)\n",
    "                \n",
    "        if epoch % eval_every == 0:\n",
    "            val_x, val_y = mnist.validation.images, mnist.validation.labels            \n",
    "            \n",
    "            feed_dict = {x : val_x, y : val_y, tf_model.keep_prob: 1.0,\n",
    "                                    tf_model.is_training: False}\n",
    "            c_eval, a_eval = session.run([loss, accuracy], feed_dict=feed_dict)\n",
    "            \n",
    "            if verbose:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"Validation acc=\", \\\n",
    "                            \"{:.9f}\".format(a_eval)\n",
    "                \n",
    "            if c_eval >= prev_c_eval and stop_early:\n",
    "                print \"Validation loss stopped improving, stopping training early after %d epochs!\" % (epoch + 1)\n",
    "                break\n",
    "                \n",
    "            prev_c_eval = c_eval\n",
    "                \n",
    "            val_costs.append((epoch, c_eval))\n",
    "            val_accs.append((epoch, a_eval))\n",
    "    \n",
    "    print \"Optimization Finished!\"\n",
    "    return train_costs, train_accs, val_costs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wAN12x1cG4eJ"
   },
   "outputs": [],
   "source": [
    "# Helper functions to plot training progress.\n",
    "\n",
    "def my_plot(list_of_tuples):\n",
    "    \"\"\"Take a list of (epoch, value) and split these into lists of \n",
    "    epoch-only and value-only. Pass these to plot to make sure we\n",
    "    line up the values at the correct time-steps.\n",
    "    \"\"\"\n",
    "    plt.plot(*zip(*list_of_tuples))\n",
    "\n",
    "def plot_multi(values_lst, labels_lst, y_label, x_label='epoch'):\n",
    "    # Plot multiple curves.\n",
    "    assert len(values_lst) == len(labels_lst)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    for v in values_lst:\n",
    "        my_plot(v)\n",
    "    plt.legend(labels_lst, loc='upper left')\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WOTTYeveZgS"
   },
   "source": [
    "Now define some helper functions to build a convolutional layer and a linear layer (this is mostly the same as the previous practical, but we use slightly different weight and bias initializations which seem to work better with ConvNets on MNIST). In terms of regularisation, we use **dropout** rather than the L2 regularisation from the previous practical. \n",
    "\n",
    "## Dropout\n",
    "Dropout is a neural-network regularisation technique that is applied during model training. At each training step, a proportion `(1-keep_prob)` of neurons the network are \"dropped out\" (their inputs and outputs are set to 0, effectively ignoring their contribution) while the remaining `keep_prob` fraction are \"let through\" (**Nit**: they're actually rescaled by `1/keep_prob` to ensure that the variance of the pre-activations at the next layer remains unchanged). This can be interpreted as there being actually $2^n$ different network architectures (where n is the number of neurons) while only one is being trained at each training step. At test time, we use the full network, where each neuron's contribution is weighted by `keep_prob`. This is effectively the average of all the network possibilities and therefore dropout can also be thought of as an ensemble technique. \n",
    "\n",
    "In our ConvNet architecture, the majority of neurons occur in the fully connected layer between the convolutional layers and the output. It is therefore this fully connected layer that we are most concerned about overfitting and this is where we apply dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "OY2RwcMh4gEr"
   },
   "outputs": [],
   "source": [
    "def _convolutional_layer(inputs, filter_size, output_depth):\n",
    "    \"\"\"Build a convolutional layer with `output_depth` square \n",
    "    filters, each of size `filter_size` x `filter_size`.\"\"\"\n",
    "\n",
    "    input_features = inputs.shape[3] \n",
    "    \n",
    "    weights = tf.get_variable(\n",
    "            \"conv_weights\", \n",
    "            [filter_size, filter_size, input_features, output_depth], \n",
    "            dtype=tf.float32,\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "     \n",
    "    ## IMPLEMENT-ME CANDIDATE\n",
    "    conv = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HB6jzZd6-XLp"
   },
   "outputs": [],
   "source": [
    "def _dense_linear_layer(inputs, layer_name, input_size, output_size, weights_initializer):\n",
    "    \"\"\"\n",
    "    Builds a layer that takes a batch of inputs of size `input_size` and returns \n",
    "    a batch of outputs of size `output_size`.\n",
    "         \n",
    "    Args:\n",
    "        inputs: A `Tensor` of shape [batch_size, input_size].\n",
    "        layer_name: A string representing the name of the layer.\n",
    "        input_size: The size of the inputs\n",
    "        output_size: The size of the outputs\n",
    "        \n",
    "    Returns:\n",
    "        out, weights: tuple of layer outputs and weights.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Name scopes allow us to logically group together related variables.\n",
    "    # Setting reuse=False avoids accidental reuse of variables between different runs.\n",
    "    with tf.variable_scope(layer_name, reuse=False):\n",
    "        # Create the weights for the layer\n",
    "        layer_weights = tf.get_variable(\"weights\",\n",
    "                                        shape=[input_size, output_size], \n",
    "                                        dtype=tf.float32, \n",
    "                                        initializer=weights_initializer)\n",
    "        # Create the biases for the layer\n",
    "        layer_bias = tf.get_variable(\"biases\", \n",
    "                                     shape=[output_size], \n",
    "                                     dtype=tf.float32, \n",
    "                                     initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        outputs = tf.matmul(inputs, layer_weights) + layer_bias\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTChH7A0h0sD"
   },
   "source": [
    "Now build the ConvNetClassifier, we make the number of convolutional layers and the filter sizes parameters so that you can easily experiment with different variations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "STEy0-DO4Cfm"
   },
   "outputs": [],
   "source": [
    "class ConvNetClassifier(BaseSoftmaxClassifier):\n",
    "    def __init__(self, \n",
    "                 input_size,     # The size of the input\n",
    "                 output_size,    # The size of the output\n",
    "                 filter_sizes=[], # The number of filters to use per convolutional layer\n",
    "                 output_depths=[], # The number of features to output per convolutional layer\n",
    "                 hidden_linear_size=512,    # The size of the hidden linear layer\n",
    "                 use_batch_norm=False,     # Flag indicating whether or not to use batch normalisation\n",
    "                 linear_weights_initializer=tf.truncated_normal_initializer(stddev=0.1)):        \n",
    "        \n",
    "        assert len(filter_sizes) == len(output_depths)\n",
    "        \n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.output_depths = output_depths\n",
    "        self.linear_weights_initializer = linear_weights_initializer\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.hidden_linear_size = hidden_linear_size\n",
    "        \n",
    "        super(ConvNetClassifier, self).__init__(input_size, output_size)\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        # Architecture: INPUT - {CONV - RELU - POOL}*N - FC\n",
    "        \n",
    "        # Reshape the input to [batch_size, width, height, input_depth]\n",
    "        conv_input = tf.reshape(self.x, [-1, 28, 28, 1])    \n",
    "        \n",
    "        prev_inputs = conv_input\n",
    "        \n",
    "        # Create the CONV-RELU-POOL layers:\n",
    "        for layer_number, (layer_filter_size, layer_features) in enumerate(\n",
    "                zip(self.filter_sizes, self.output_depths)):\n",
    "            with tf.variable_scope(\"layer_{}\".format(layer_number), reuse=False):\n",
    "                # Create the convolution:\n",
    "                conv = _convolutional_layer(prev_inputs, layer_filter_size, layer_features)\n",
    "                \n",
    "                # Apply batch normalisation, if required\n",
    "                if self.use_batch_norm:\n",
    "                    conv = tf.contrib.layers.batch_norm(conv, center=True, scale=True, \n",
    "                                                        is_training=self.is_training)\n",
    "                \n",
    "                # Apply the RELU activation with a bias\n",
    "                bias = tf.get_variable(\"bias\", [layer_features], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "                relu = tf.nn.relu(conv + bias)\n",
    "                \n",
    "                # Apply max-pooling using patch-sizes of 2x2\n",
    "                pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1],\n",
    "                                                            strides=[1, 2, 2, 1], \n",
    "                                                            padding='SAME')\n",
    "                \n",
    "                # QUESTION: What is the shape of the pool tensor?\n",
    "                # ANSWER: ...\n",
    "                \n",
    "                prev_inputs = pool\n",
    "        \n",
    "        \n",
    "        # QUESTION: What is the shape of prev_inputs now? \n",
    "        \n",
    "        # We need to flatten the last (non-batch) dimensions of the convolutional\n",
    "        # output in order to pass it to a fully-connected layer:\n",
    "        flattened = tf.contrib.layers.flatten(prev_inputs)\n",
    "        \n",
    "        # Create the fully-connected (linear) layer that maps the flattened inputs\n",
    "        # to `hidden_linear_size` hidden outputs\n",
    "        flat_size = flattened.shape[1]\n",
    "        fully_connected = _dense_linear_layer(\n",
    "                flattened, \"fully_connected\", flat_size, self.hidden_linear_size, self.linear_weights_initializer)\n",
    "        \n",
    "        # Apply batch normalisation, if required\n",
    "        if self.use_batch_norm:\n",
    "            fully_connected = tf.contrib.layers.batch_norm(\n",
    "                    fully_connected, center=True, scale=True, is_training=self.is_training)\n",
    "        \n",
    "        fc_relu = tf.nn.relu(fully_connected)\n",
    "        \n",
    "        fc_drop = tf.nn.dropout(fc_relu, self.keep_prob)\n",
    "        \n",
    "        # Now we map the `hidden_linear_size` outputs to the `output_size` logits, one for each possible digit class\n",
    "        logits = _dense_linear_layer(\n",
    "                fc_drop, \"logits\", self.hidden_linear_size, self.output_size, self.linear_weights_initializer)\n",
    "        \n",
    "        self.logits = logits\n",
    "        self.predictions = tf.nn.softmax(self.logits)\n",
    "        self.loss = self.compute_loss()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZPkvle1RDfk"
   },
   "source": [
    "Finally a function that wraps up the training and evaluation of the model (the same as prac 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "rYuK9io1RJ2w"
   },
   "outputs": [],
   "source": [
    "def build_train_eval_and_plot(build_params, train_params, verbose=True):\n",
    "    tf.reset_default_graph()\n",
    "    m = ConvNetClassifier(**build_params)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Train model on the MNIST dataset.\n",
    "     \n",
    "        train_losses, train_accs, val_losses, val_accs = train_tf_model(\n",
    "                m, \n",
    "                sess,\n",
    "                verbose=verbose,\n",
    "                **train_params) \n",
    "        \n",
    "        # Now evaluate it on the test set:\n",
    "    \n",
    "        accuracy_op = m.accuracy()    # Get the symbolic accuracy operation\n",
    "        # Calculate the accuracy using the test images and labels.\n",
    "        accuracy = accuracy_op.eval({m.x: mnist.test.images, \n",
    "                                     m.y: mnist.test.labels,\n",
    "                                     m.keep_prob: 1.0,\n",
    "                                     m.is_training: False})    \n",
    "        \n",
    "        if verbose: \n",
    "            print \"Accuracy on test set:\", accuracy\n",
    "            # Plot losses and accuracies.\n",
    "            plot_multi([train_losses, val_losses], ['train', 'val'], 'loss', 'epoch')\n",
    "            plot_multi([train_accs, val_accs], ['train', 'val'], 'accuracy', 'epoch')\n",
    "            \n",
    "        \n",
    "        ret = {'train_losses': train_losses, 'train_accs' : train_accs,\n",
    "               'val_losses' : val_losses, 'val_accs' : val_accs,\n",
    "               'test_acc' : accuracy}\n",
    "        \n",
    "        # Evaluate the final convolutional weights\n",
    "        conv_variables = [v for v in tf.trainable_variables() if \"conv_weights\" in v.name]\n",
    "        conv_weights = sess.run(conv_variables)\n",
    "        \n",
    "        return m, ret, conv_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03Pbb466Bdrh"
   },
   "source": [
    "Now train and evaluate the ConvNet model on MNIST. \n",
    "\n",
    "**NOTE**: Hopefully you answered the question in the first section about the tradeoffs a ConvNet makes with \"extra computation\"! Unfortuntaly the VMs we're using are pretty low-powered, so we will train a very small ConvNet just to check that it works. Once you've got this small ConvNet working, chat to a tutor to get access to a machine with a GPU and run with the following configuration to get to the promised 99%+ accuracy on MNIST!\n",
    "\n",
    "```\n",
    "model_params = {\n",
    "        'input_size': 784,\n",
    "        'output_size': 10,\n",
    "        'filter_sizes': [5, 5],\n",
    "        'output_depths': [32, 64],\n",
    "        'hidden_linear_size': 1024,\n",
    "        'use_batch_norm': False\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "        'keep_prob': 0.5,\n",
    "        'num_epochs': 20,\n",
    "        'batch_size': 50,\n",
    "        'stop_early': False,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 1273,
     "output_extras": [
      {
       "item_id": 31
      },
      {
       "item_id": 32
      },
      {
       "item_id": 33
      },
      {
       "item_id": 34
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 290757,
     "status": "ok",
     "timestamp": 1504368810426,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "_KQq51S9Bc73",
    "outputId": "2141f1f9-c30c-47a2-9525-fb1e42834a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training cost= 1.168772998\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[5000,28,28,4]\n\t [[Node: layer_0/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, layer_0/conv_weights/read)]]\n\nCaused by op u'layer_0/Conv2D', defined at:\n  File \"/home/p2/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/p2/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-d6c676f407b8>\", line 1, in <module>\n    get_ipython().run_cell_magic(u'time', u'', u\"\\n# Create a ConvNet classifier with 2 CONV-RELU-POOL layers, with 7x7 filters, \\n# 32 and 64 output features and a hidden linear layer size of 512.\\nmodel_params = {\\n        'input_size': 784,\\n        'output_size': 10,\\n        'filter_sizes': [5],\\n        'output_depths': [4],\\n        'hidden_linear_size': 128,\\n        'use_batch_norm': False\\n}\\n\\ntraining_params = {\\n        'keep_prob': 0.5,\\n        'num_epochs': 5,\\n        'batch_size': 50,\\n        'stop_early': False,\\n}\\n\\ntrained_model, training_results, conv_weights = build_train_eval_and_plot(\\n        model_params, \\n        training_params, \\n        verbose=True\\n)\")\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2115, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-59>\", line 2, in time\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.py\", line 1185, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 23, in <module>\n  File \"<ipython-input-16-ec512f958ac8>\", line 3, in build_train_eval_and_plot\n    m = ConvNetClassifier(**build_params)\n  File \"<ipython-input-15-37eb57c918fe>\", line 19, in __init__\n    super(ConvNetClassifier, self).__init__(input_size, output_size)\n  File \"<ipython-input-10-e0e75fbc595d>\", line 22, in __init__\n    self.build_model()\n  File \"<ipython-input-15-37eb57c918fe>\", line 35, in build_model\n    conv = _convolutional_layer(prev_inputs, layer_filter_size, layer_features)\n  File \"<ipython-input-13-87685a4b2adb>\", line 14, in _convolutional_layer\n    conv = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding='SAME')\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[5000,28,28,4]\n\t [[Node: layer_0/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, layer_0/conv_weights/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d6c676f407b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"\\n# Create a ConvNet classifier with 2 CONV-RELU-POOL layers, with 7x7 filters, \\n# 32 and 64 output features and a hidden linear layer size of 512.\\nmodel_params = {\\n        'input_size': 784,\\n        'output_size': 10,\\n        'filter_sizes': [5],\\n        'output_depths': [4],\\n        'hidden_linear_size': 128,\\n        'use_batch_norm': False\\n}\\n\\ntraining_params = {\\n        'keep_prob': 0.5,\\n        'num_epochs': 5,\\n        'batch_size': 50,\\n        'stop_early': False,\\n}\\n\\ntrained_model, training_results, conv_weights = build_train_eval_and_plot(\\n        model_params, \\n        training_params, \\n        verbose=True\\n)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-ec512f958ac8>\u001b[0m in \u001b[0;36mbuild_train_eval_and_plot\u001b[0;34m(build_params, train_params, verbose)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 **train_params) \n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Now evaluate it on the test set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7995852549a3>\u001b[0m in \u001b[0;36mtrain_tf_model\u001b[0;34m(tf_model, session, num_epochs, batch_size, keep_prob, optimizer_fn, report_every, eval_every, stop_early, verbose)\u001b[0m\n\u001b[1;32m     76\u001b[0m             feed_dict = {x : val_x, y : val_y, tf_model.keep_prob: 1.0,\n\u001b[1;32m     77\u001b[0m                                     tf_model.is_training: False}\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mc_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[5000,28,28,4]\n\t [[Node: layer_0/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, layer_0/conv_weights/read)]]\n\nCaused by op u'layer_0/Conv2D', defined at:\n  File \"/home/p2/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/p2/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-d6c676f407b8>\", line 1, in <module>\n    get_ipython().run_cell_magic(u'time', u'', u\"\\n# Create a ConvNet classifier with 2 CONV-RELU-POOL layers, with 7x7 filters, \\n# 32 and 64 output features and a hidden linear layer size of 512.\\nmodel_params = {\\n        'input_size': 784,\\n        'output_size': 10,\\n        'filter_sizes': [5],\\n        'output_depths': [4],\\n        'hidden_linear_size': 128,\\n        'use_batch_norm': False\\n}\\n\\ntraining_params = {\\n        'keep_prob': 0.5,\\n        'num_epochs': 5,\\n        'batch_size': 50,\\n        'stop_early': False,\\n}\\n\\ntrained_model, training_results, conv_weights = build_train_eval_and_plot(\\n        model_params, \\n        training_params, \\n        verbose=True\\n)\")\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2115, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-59>\", line 2, in time\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.py\", line 1185, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 23, in <module>\n  File \"<ipython-input-16-ec512f958ac8>\", line 3, in build_train_eval_and_plot\n    m = ConvNetClassifier(**build_params)\n  File \"<ipython-input-15-37eb57c918fe>\", line 19, in __init__\n    super(ConvNetClassifier, self).__init__(input_size, output_size)\n  File \"<ipython-input-10-e0e75fbc595d>\", line 22, in __init__\n    self.build_model()\n  File \"<ipython-input-15-37eb57c918fe>\", line 35, in build_model\n    conv = _convolutional_layer(prev_inputs, layer_filter_size, layer_features)\n  File \"<ipython-input-13-87685a4b2adb>\", line 14, in _convolutional_layer\n    conv = tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding='SAME')\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/p2/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[5000,28,28,4]\n\t [[Node: layer_0/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, layer_0/conv_weights/read)]]\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Create a ConvNet classifier with 2 CONV-RELU-POOL layers, with 7x7 filters, \n",
    "# 32 and 64 output features and a hidden linear layer size of 512.\n",
    "model_params = {\n",
    "        'input_size': 784,\n",
    "        'output_size': 10,\n",
    "        'filter_sizes': [5],\n",
    "        'output_depths': [4],\n",
    "        'hidden_linear_size': 128,\n",
    "        'use_batch_norm': False\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "        'keep_prob': 0.5,\n",
    "        'num_epochs': 5,\n",
    "        'batch_size': 50,\n",
    "        'stop_early': False,\n",
    "}\n",
    "\n",
    "trained_model, training_results, conv_weights = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC9-p4EuSSyc"
   },
   "source": [
    "The ConvNet classifier takes quite a long time to train, but gives a very respectable test accuracy of over **99%**! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vV1i8ilwElbp"
   },
   "source": [
    "## What has the network learned?\n",
    "Remember that a filter in a convolutional layer is used to multiply blocks in the input volume. Let's plot the weights of the first layer of the trained model. Darker pixels indicate that the particular filter reacts more strongly to those regions of the input blocks. Notice how each filter has learned to react differently to different patterns in the input. It's tricky to see in our tiny filters, but those in lower layers of ConvNets, particularly when applied to natural images, often function as simple Gabor filters or edge detectors, while filters in higher layers often react to more abstract shapes and concepts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 272,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2702,
     "status": "ok",
     "timestamp": 1504370167320,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "Bcr3zmMnQkC-",
    "outputId": "535554e9-cf7a-47e2-d17a-e54e3ba5cfa1"
   },
   "outputs": [],
   "source": [
    "weights = conv_weights[0]\n",
    "_, _, _, out_depth = weights.shape\n",
    "grid_size = int(out_depth**0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "i = 1\n",
    "for r in range(grid_size):\n",
    "    for c in range(grid_size):\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i)\n",
    "        ax.imshow(weights[:, :, 0, r*grid_size+c], cmap=\"Greys\")\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNjw-0cBcfah"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPehk6-WXLvo"
   },
   "source": [
    "# Aside: The Effect of Random Initialization - RUN THIS ON A GPU INSTANCE ONLY!\n",
    "Initialization of model parameters matters! Here is a ConvNet with different, but seemingly sensible, initialization of the weights in the linear layer. Running this gives significantly worse results. Judging by the accuracy plot, it's possible that training this model long enough will get it to a simliar level as before, but it will take much longer. This shows that initialization of model parameters is a an important consideration, especially as models become more complex. In practice, there are a number of different initialization schemes to consider. In particular, [Xavier](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) tends to work well with ConvNets and is worth considering. We won't go into any details in this practical though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 1273,
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 42
      },
      {
       "item_id": 43
      },
      {
       "item_id": 44
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1771121,
     "status": "ok",
     "timestamp": 1503659151747,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "grDD7jGUXXpF",
    "outputId": "eea0addd-c8c1-4ba5-ed86-197eafdb392f"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Create a ConvNet classifier with 2 CONV-RELU-POOL layers, with filter sizes of \n",
    "# 5 and 5 and 32 and 64 output features.\n",
    "model_params = {\n",
    "        'input_size': 784,\n",
    "        'output_size': 10,\n",
    "        'filter_sizes': [5, 5],\n",
    "        'output_depths': [32, 64],\n",
    "        'hidden_linear_size': 1024,\n",
    "        'use_batch_norm': False\n",
    "        'linear_weights_initializer': tf.random_normal_initializer()\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "        'keep_prob': 0.5,\n",
    "        'num_epochs': 5,\n",
    "        'batch_size': 50,\n",
    "        'stop_early': False,\n",
    "}\n",
    "\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVuVQoUSogFA"
   },
   "source": [
    "## Batch Normalisation\n",
    "Batch normalisation (batch norm) is a more recent (2015) and arguably more powerful normalisation technique than dropout. It is based on the observation that machine learning models often perform better and train faster when their inputs are normalised to have 0 mean and unit variance. In multi-layered deep neural networks, the output of one layer becomes the input to the next. The insight behind batch norm is that each of these layer inputs can also be normalised. Batch norm has been shown to have numerous benefits including:\n",
    "* Networks tend to train faster\n",
    "* Allows higher learning rates to be used (further improving training speed).\n",
    "* Reduced sensitivity to weight initialisation.\n",
    "* Makes certain activation functions feasible in deep networks (When inputs have very large (absolute) expected values, certain activation functions become saturated (For example, the output of sigmoid is always to 1 for large inputs). Relu activations can also \"die out\" when the expected value of the input is a large negative value (why?). This results in wasted computation as these neurons become uninformative. Normalising the inputs to have 0 mean keeps these activation functions in the \"sensible\" parts of their domains.)\n",
    "\n",
    "### How does it work? \n",
    "To normalise some inputs X, ideally we would like to set\n",
    "$\\hat X = \\frac{X - E[X]}{\\sqrt{VAR[X]}}$\n",
    "but this requires knowledge of the population mean and variance statistics, which we don't know, at least during training. We therefore use the **sample mean** and **sample variance** of each batch encountered during training as unbiased estimates of these statistics. During testing, we use statistics gathered throughout training as better estimates of the population statistics. In addition to this, we would like the model to have some flexibility over the extent to which batch norm is applied, and this flexibility should be learned! In order to do this, we introduce two new trainable parameters, $\\gamma$ and $\\beta$ for each layer that batch norm is applied to. Suppose we have a batch of inputs to a layer, $B={x_1,...,x_m}$, we normalise these as follows: \n",
    "\n",
    "$\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i$ &nbsp;&nbsp;&nbsp; (Batch mean)\n",
    "\n",
    "${\\sigma_B}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2$ &nbsp;&nbsp;&nbsp; (Batch variance)\n",
    " \n",
    "$\\hat x_i= \\frac{x_i - \\mu_B}{\\sqrt{{\\sigma_B}^2}}$ &nbsp;&nbsp;&nbsp; (Normalised)\n",
    "\n",
    "$y_i = \\gamma \\hat x_i + \\beta$ &nbsp;&nbsp;&nbsp; (Scale and shift)\n",
    "\n",
    "\n",
    "At test time, we normalise using the mean and variance computed over the entire training set:\n",
    "\n",
    "$E[x] = E_B[\\mu_B]$\n",
    "\n",
    "$VAR[x] = \\frac{m}{m-1}E_B[{\\sigma_B}^2]$\n",
    "\n",
    "$\\hat x = \\frac{x - E[x]}{\\sqrt{VAR[x]}}$\n",
    "\n",
    "$y = \\gamma \\hat x + \\beta$\n",
    "\n",
    "### Implementation Details\n",
    "Tracking the mean and variance over the training set can become a little fiddly. Many implementations also use a *moving average* of the batch mean and variance as estimates of the population mean and variance for use during testing. Luckily, TensorFlow provides batch norm out of the box in the form of the `tf.contrib.layers.batch_norm` function. \n",
    "\n",
    "Since the behaviour of batch norm changes during training and testing, we need to pass a placeholder input to the function that indicates which phase we are in. Furthermore, the batch norm function uses variable updates to track the moving average mean and variance. These values are not used during training and so TensorFlow's graph execution logic will not naturally run these updates when you run a training step. In order to get around this, the `batch_norm` function adds these update ops to a *graph collection* that we can access in our training function. The following code, which you will see in the `train_tf_model` function retrieves these ops and then adds a *control dependency* to the optimiser step. This effectively tells TensorFlow that the update_ops must be run before the optimizer_step can be run, ensuring that the estimates are updated whenever we do a training step. \n",
    "\n",
    "```\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer_step = optimizer_fn.minimize(loss)\n",
    "```\n",
    "\n",
    "Further choices to consider when using batch norm are where to apply it (some apply it immediately before each activation function, some *after* the activation function), whether to apply it to all layers and whether or not to *share* the gamma and beta parameters over all layers or have separate values for each layer. . \n",
    "\n",
    "Have a look at the ConvNetClassifer class above to see what choices were made, try changing these and see what results you get! (See the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm) for a list of even more parameters you can experiment with)\n",
    "\n",
    "Now, finally, let's switch batch norm on and see how our ConvNetClassifier performs. (Note: we shouldn't expect it to necessarily perform better than dropout as we are already close to the limits of how well we can classify MNIST with our relatively small ConvNet!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 1103,
     "output_extras": [
      {
       "item_id": 30
      },
      {
       "item_id": 31
      },
      {
       "item_id": 32
      },
      {
       "item_id": 33
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 871282,
     "status": "ok",
     "timestamp": 1503578600521,
     "user": {
      "displayName": "Avishkar Bhoopchand",
      "photoUrl": "//lh5.googleusercontent.com/-OainnMWSi6A/AAAAAAAAAAI/AAAAAAAAAOQ/eI7Z19q5v7E/s50-c-k-no/photo.jpg",
      "userId": "105781587643595215149"
     },
     "user_tz": -60
    },
    "id": "yGv9t5hdoTWF",
    "outputId": "e5124d04-3688-44ac-dd9d-19f0c5556bc2"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## UNFORTUNATELY THIS WILL ALSO NOT WORK ON THE VM's, YOU'LL NEED TO GET A GPU INSTANCE TO RUN THIS!\n",
    "\n",
    "# Create a ConvNet classifier with 2 CONV-RELU-POOL layers, with filter sizes of \n",
    "# 5 and 5 and 32 and 64 output features.\n",
    "model_params = {\n",
    "        'input_size': 784,\n",
    "        'output_size': 10,\n",
    "        'filter_sizes': [5, 5],\n",
    "        'output_depths': [32, 64],\n",
    "        'use_batch_norm': True,\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "        'keep_prob': 1.0,    # Switch off dropout\n",
    "        'num_epochs': 15,\n",
    "        'batch_size': 50,\n",
    "        'stop_early': False,\n",
    "}\n",
    "\n",
    "trained_model, training_results = build_train_eval_and_plot(\n",
    "        model_params, \n",
    "        training_params, \n",
    "        verbose=True\n",
    ")\n",
    "\n",
    "# QUESTION: Try experimenting with different archictures and hyperparameters and \n",
    "# see how well you can classify MNIST digits! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SgtWksRT4hJ"
   },
   "source": [
    "# NB: Before you go (5min)\n",
    "\n",
    "Pair up with someone else and go through the questions in \"Learning Objectives\" at the top. Take turns explaining each of these to each other, and be sure to ask the tutors if you're both unsure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxEVtIbcovwl"
   },
   "source": [
    "# Additional resources\n",
    "\n",
    "* Chip Huyen's wonderful script: https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/07_convnet_mnist.py\n",
    "* NVidia's ConvNet Tutorial: https://github.com/alrojo/tensorflow-tutorial/blob/master/lab2_CNN/lab2_CNN.ipynb\n",
    "* https://github.com/DeepLearningDTU/nvidia_deep_learning_summercamp_2016/blob/master/lab2/lab2_CNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lJYoEZV32p_"
   },
   "source": [
    "# Feedback\n",
    "\n",
    "Please send any bugs and comments to dli-practicals@googlegroups.com."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "//learning/deepmind/dm_python:dm_notebook",
    "kind": "private"
   },
   "name": "Prac 3: Deep ConvNets (Solution)",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
